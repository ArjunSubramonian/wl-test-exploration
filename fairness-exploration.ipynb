{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac78d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_credit, load_german, feature_norm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import convert, to_networkx\n",
    "import torch\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5a1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset):\n",
    "    # Load credit_scoring dataset\n",
    "    if dataset == 'credit':\n",
    "        sens_attr = \"Age\"  # column number after feature process is 1\n",
    "        sens_idx = 1\n",
    "        predict_attr = 'NoDefaultNextMonth'\n",
    "        label_number = 30000\n",
    "        path_credit = \"./dataset/credit\"\n",
    "        adj, features, labels, idx_train, idx_val, idx_test, sens = load_credit(dataset, sens_attr,\n",
    "                                                                                predict_attr, path=path_credit,\n",
    "                                                                                label_number=label_number\n",
    "                                                                                )\n",
    "        norm_features = feature_norm(features)\n",
    "        norm_features[:, sens_idx] = features[:, sens_idx]\n",
    "        features = norm_features\n",
    "\n",
    "    # Load german dataset\n",
    "    elif dataset == 'german':\n",
    "        sens_attr = \"Gender\"  # column number after feature process is 0\n",
    "        sens_idx = 0\n",
    "        predict_attr = \"GoodCustomer\"\n",
    "        label_number = 1000\n",
    "        path_german = \"./dataset/german\"\n",
    "        adj, features, labels, idx_train, idx_val, idx_test, sens = load_german(dataset, sens_attr,\n",
    "                                                                                predict_attr, path=path_german,\n",
    "                                                                                label_number=label_number,\n",
    "                                                                                )\n",
    "    \n",
    "    edge_index = convert.from_scipy_sparse_matrix(adj)[0]\n",
    "    # don't include sensitive attributes\n",
    "    features = torch.cat((features[:, :sens_idx], features[:, sens_idx+1:]), dim=1)\n",
    "    sens = torch.tensor(sens).long()\n",
    "    \n",
    "    data = Data(\n",
    "            x=features,\n",
    "            edge_index=edge_index,\n",
    "            y=labels.long(),\n",
    "            edge_attr=torch.ones(edge_index.size(1)),\n",
    "        )\n",
    "    num_nodes = features.size(0)\n",
    "    data.train_mask = idx_train\n",
    "    data.val_mask = idx_val\n",
    "    data.test_mask = idx_test\n",
    "\n",
    "    dataset = types.SimpleNamespace()\n",
    "    dataset.data = data\n",
    "    dataset.num_classes = data.y.max().item() + 1\n",
    "\n",
    "    return dataset, sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472cf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from hashlib import blake2b\n",
    "\n",
    "def _hash_label(label, digest_size):\n",
    "    return blake2b(label.encode(\"ascii\"), digest_size=digest_size).hexdigest()\n",
    "\n",
    "\n",
    "def _init_node_labels(G, edge_attr, node_attr):\n",
    "    if node_attr:\n",
    "        return {u: str(dd[node_attr]) for u, dd in G.nodes(data=True)}\n",
    "    elif edge_attr:\n",
    "        return {u: \"\" for u in G}\n",
    "    else:\n",
    "        # use same hash for all nodes if no features\n",
    "        return {u: \"0\" for u, deg in G.degree()}\n",
    "\n",
    "\n",
    "def _neighborhood_aggregate(G, node, node_labels, edge_attr=None):\n",
    "    \"\"\"\n",
    "    Compute new labels for given node by aggregating\n",
    "    the labels of each node's neighbors.\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    for nbr in G.neighbors(node):\n",
    "        prefix = \"\" if edge_attr is None else str(G[node][nbr][edge_attr])\n",
    "        label_list.append(prefix + node_labels[nbr])\n",
    "    return node_labels[node] + \"\".join(sorted(label_list))\n",
    "\n",
    "\n",
    "def weisfeiler_lehman_graph_hash(\n",
    "    G, edge_attr=None, node_attr=None, iterations=3, digest_size=16\n",
    "):\n",
    "    def weisfeiler_lehman_step(G, labels, edge_attr=None):\n",
    "        \"\"\"\n",
    "        Apply neighborhood aggregation to each node\n",
    "        in the graph.\n",
    "        Computes a dictionary with labels for each node.\n",
    "        \"\"\"\n",
    "        new_labels = {}\n",
    "        for node in G.nodes():\n",
    "            label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n",
    "            new_labels[node] = _hash_label(label, digest_size)\n",
    "        return new_labels\n",
    "\n",
    "    # set initial node labels\n",
    "    node_labels = _init_node_labels(G, edge_attr, node_attr)\n",
    "\n",
    "    subgraph_hash_counts = []\n",
    "    for _ in range(iterations):\n",
    "        node_labels = weisfeiler_lehman_step(G, node_labels, edge_attr=edge_attr)\n",
    "        counter = Counter(node_labels.values())\n",
    "        # sort the counter, extend total counts\n",
    "        subgraph_hash_counts.extend(sorted(counter.items(), key=lambda x: x[0]))\n",
    "\n",
    "    # hash the final counter\n",
    "    return _hash_label(str(tuple(subgraph_hash_counts)), digest_size)\n",
    "\n",
    "def weisfeiler_lehman_subgraph_hashes(\n",
    "    G, edge_attr=None, node_attr=None, iterations=3, digest_size=16\n",
    "):\n",
    "    def weisfeiler_lehman_step(G, labels, node_subgraph_hashes, edge_attr=None):\n",
    "        \"\"\"\n",
    "        Apply neighborhood aggregation to each node\n",
    "        in the graph.\n",
    "        Computes a dictionary with labels for each node.\n",
    "        Appends the new hashed label to the dictionary of subgraph hashes\n",
    "        originating from and indexed by each node in G\n",
    "        \"\"\"\n",
    "        new_labels = {}\n",
    "        for node in G.nodes():\n",
    "            label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n",
    "            hashed_label = _hash_label(label, digest_size)\n",
    "            new_labels[node] = hashed_label\n",
    "            node_subgraph_hashes[node].append(hashed_label)\n",
    "        return new_labels\n",
    "\n",
    "    node_labels = _init_node_labels(G, edge_attr, node_attr)\n",
    "\n",
    "    node_subgraph_hashes = defaultdict(list)\n",
    "    for _ in range(iterations):\n",
    "        node_labels = weisfeiler_lehman_step(\n",
    "            G, node_labels, node_subgraph_hashes, edge_attr\n",
    "        )\n",
    "\n",
    "    return dict(node_subgraph_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95d6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_hashes(dataset):\n",
    "    partitions = [] # Found partitions\n",
    "    for G_idx, y_idx, _hash in dataset: # Loop over each element\n",
    "        found = False # Note it is not yet part of a know partition\n",
    "        for p in partitions:\n",
    "            if _hash == p[0][2]: # Found a partition for it!\n",
    "                p.append((G_idx, y_idx, _hash))\n",
    "                found = True\n",
    "                break\n",
    "        if not found: # Make a new partition for it.\n",
    "            partitions.append([(G_idx, y_idx, _hash)])\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed1586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wl-test-exploration/utils.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  idx_features_labels['Gender'][idx_features_labels['Gender'] == 'Female'] = 1\n",
      "/workspace/wl-test-exploration/utils.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  idx_features_labels['Gender'][idx_features_labels['Gender'] == 'Male'] = 0\n",
      "/workspace/wl-test-exploration/utils.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  idx_features_labels['PurposeOfLoan'][idx_features_labels['PurposeOfLoan'] == val] = i\n"
     ]
    }
   ],
   "source": [
    "loaded_datasets = {}\n",
    "for dataset_name in [\"credit\", \"german\"]:\n",
    "    loaded_datasets[dataset_name] = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24722fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_class_stats(dataset, name, partition):\n",
    "    print('num', name, 'eq classes:', len(partition))\n",
    "    print('num singleton', name, 'classes:', sum([int(len(c) == 1) for c in partition]))\n",
    "    return [len(partition), sum([int(len(c) == 1) for c in partition])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20099f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== credit ======\n",
      "num wl-3 eq classes: 29535\n",
      "num singleton wl-3 classes: 29367\n",
      "====== credit-0 ======\n",
      "num wl-3 eq classes: 26874\n",
      "num singleton wl-3 classes: 26720\n",
      "====== credit-1 ======\n",
      "num wl-3 eq classes: 2662\n",
      "num singleton wl-3 classes: 2649\n",
      "====== german ======\n",
      "num wl-3 eq classes: 1000\n",
      "num singleton wl-3 classes: 1000\n"
     ]
    }
   ],
   "source": [
    "wl_stats = {}\n",
    "\n",
    "for dataset_name in [\"credit\", \"credit-0\", \"credit-1\", \"german\"]:\n",
    "    \n",
    "    dn = dataset_name\n",
    "    sens_type = None\n",
    "    if \"-\" in dataset_name:\n",
    "        dn, sens_type = dataset_name.split(\"-\")\n",
    "        sens_type = int(sens_type)\n",
    "    \n",
    "    dataset, sens = loaded_datasets[dn]\n",
    "    \n",
    "    print(\"======\", dataset_name, \"======\")\n",
    "    \n",
    "    hashed_dataset = []\n",
    "    k = 3\n",
    "    G_idx = 0\n",
    "    G_torch = dataset.data\n",
    "    G_nx = to_networkx(G_torch, node_attrs=['x'])\n",
    "    G_hashes = weisfeiler_lehman_subgraph_hashes(G_nx, node_attr='x', iterations=k)\n",
    "    for node_id in G_hashes:\n",
    "        if sens_type is not None and sens[node_id].item() != sens_type:\n",
    "            continue\n",
    "        hashed_dataset.append((G_idx, node_id, G_hashes[node_id][-1]))\n",
    "\n",
    "    stats = [len(hashed_dataset)]\n",
    "    wl_eq_classes = partition_hashes(hashed_dataset)\n",
    "    stats.extend(eq_class_stats(dataset, \"wl-{}\".format(k), wl_eq_classes))\n",
    "\n",
    "    cols = [r'\\# nodes', r'$|{\\cal E}_{\\text{WL}}^' + str(k) + r'|$', r'$\\#_1 ({\\cal E}_{\\text{WL}}^' + str(k) + r')$']\n",
    "        \n",
    "    wl_stats[dataset_name] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6460183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  \\# nodes &  $|{\\cal E}_{\\text{WL}}^3|$ &  $\\#_1 ({\\cal E}_{\\text{WL}}^3)$ \\\\\n",
      "\\midrule\n",
      "credit   &     30000 &                       29535 &                            29367 \\\\\n",
      "credit-0 &     27315 &                       26874 &                            26720 \\\\\n",
      "credit-1 &      2685 &                        2662 &                             2649 \\\\\n",
      "german   &      1000 &                        1000 &                             1000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_wl_stats = pd.DataFrame.from_dict(wl_stats, orient='index', columns=cols)\n",
    "print(pd_wl_stats.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bc0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
